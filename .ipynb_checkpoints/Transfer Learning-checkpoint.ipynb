{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (models.py, line 33)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\Maheshwaran\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3331\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-05eb69cd4950>\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from models import get_model\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Maheshwaran\\Downloads\\Detecting-Cyberbullying-Across-SMPs-master\\Detecting-Cyberbullying-Across-SMPs-master\\models.py\"\u001b[1;36m, line \u001b[1;32m33\u001b[0m\n\u001b[1;33m    print model.summary()\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from models import get_model\n",
    "import argparse\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import preprocessor as p\n",
    "from collections import Counter\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from tensorflow.contrib import learn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from scipy import stats\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ 'cnn', 'lstm', 'blstm', 'blstm_attention']\n",
    "word_vectors = [\"random\", \"glove\" ,\"sswe\"]\n",
    "EMBED_SIZE = 50\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "MAX_FEATURES = 2\n",
    "NUM_CLASSES = 2\n",
    "DROPOUT = 0.25\n",
    "LEARN_RATE = 0.01\n",
    "HASH_REMOVE=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    print(\"Loading data from file: \" + filename)\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = [] \n",
    "    for i in range(len(data)):\n",
    "        if(HASH_REMOVE):\n",
    "            x_text.append(p.tokenize((data[i]['text']).encode('utf-8')))\n",
    "        else:\n",
    "            x_text.append(data[i]['text'])\n",
    "        labels.append(data[i]['label'])\n",
    "    return x_text,labels\n",
    "\n",
    "def get_filename(dataset):\n",
    "    global HASH_REMOVE\n",
    "    if(dataset==\"twitter\"):\n",
    "        HASH_REMOVE = True\n",
    "        EPOCHS = 10\n",
    "        BATCH_SIZE = 128\n",
    "        MAX_FEATURES = 2\n",
    "        filename = \"data/twitter_data.pkl\"\n",
    "    elif(dataset==\"formspring\"):\n",
    "        HASH_REMOVE = False\n",
    "        EPOCHS = 10\n",
    "        BATCH_SIZE = 128\n",
    "        MAX_FEATURES = 2\n",
    "        filename = \"data/formspring_data.pkl\"\n",
    "    elif(dataset==\"wiki\"):\n",
    "        HASH_REMOVE = False\n",
    "        EPOCHS = 5\n",
    "        BATCH_SIZE = 512\n",
    "        MAX_FEATURES = 5\n",
    "        filename = \"data/wiki_data.pkl\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-845e7bbbce5b>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-845e7bbbce5b>\"\u001b[1;36m, line \u001b[1;32m38\u001b[0m\n\u001b[1;33m    print \"ERROR: Please specify a correst model or SSWE cannot be loaded with embed size of: \" + str(emb_dim)\u001b[0m\n\u001b[1;37m                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_weights(filename, sep):\n",
    "    embed_dict = {}\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(sep)\n",
    "        embed_dict[row[0]] = row[1:]\n",
    "    print('Loaded from file: ' + str(filename))\n",
    "    file.close()\n",
    "    return embed_dict\n",
    "\n",
    "def map_embedding_weights(embed, vocab, embed_size):\n",
    "    vocab_size = len(vocab)\n",
    "    embeddingWeights = np.zeros((vocab_size , embed_size))\n",
    "    n = 0\n",
    "    words_missed = []\n",
    "    for k, v in vocab.iteritems():\n",
    "        try:\n",
    "            embeddingWeights[v] = embed[k]\n",
    "        except:\n",
    "            n += 1\n",
    "            words_missed.append(k)\n",
    "            pass\n",
    "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
    "    return embeddingWeights\n",
    "\n",
    "def get_embeddings_dict(vector_type, emb_dim, data):\n",
    "    if vector_type == 'sswe':\n",
    "        emb_dim==50\n",
    "        sep = '\\t'\n",
    "        vector_file = 'word_vectors/sswe-u.txt'\n",
    "    elif vector_type ==\"glove\":\n",
    "        sep = ' '\n",
    "        if data == \"wiki\":\n",
    "            vector_file = 'word_vectors/glove.6B.' + str(emb_dim) + 'd.txt'\n",
    "        else:\n",
    "            vector_file = 'word_vectors/glove.twitter.27B.' + str(emb_dim) + 'd.txt'\n",
    "    else:\n",
    "        print \"ERROR: Please specify a correst model or SSWE cannot be loaded with embed size of: \" + str(emb_dim) \n",
    "        return None\n",
    "    \n",
    "    embed = get_embedding_weights(vector_file, sep)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, testX, testY):\n",
    "    temp = model.predict(testX)\n",
    "    y_pred  = np.argmax(temp, 1)\n",
    "    y_true = np.argmax(testY, 1)\n",
    "    precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "    recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "    f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "    print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "    print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "    print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\":: Classification Report\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(precision_scores, recall_scores, f1_scores):\n",
    "    for i in range(NUM_CLASSES):\n",
    "        print(\"\\nPrecision Class %d (avg): %0.3f (+/- %0.3f)\" % (i, precision_scores[:, i].mean(), precision_scores[:, i].std() * 2))\n",
    "        print( \"\\nRecall Class %d (avg): %0.3f (+/- %0.3f)\" % (i, recall_scores[:, i].mean(), recall_scores[:, i].std() * 2))\n",
    "        print( \"\\nF1 score Class %d (avg): %0.3f (+/- %0.3f)\" % (i, f1_scores[:, i].mean(), f1_scores[:, i].std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, oversampling_rate):\n",
    "    \n",
    "    x_text, labels = load_data(get_filename(data)) \n",
    " \n",
    "    if(data==\"twitter\"):\n",
    "        dict1 = {'racism':1,'sexism':1,'none':0} #Transfer learning only two classes\n",
    "        labels = [dict1[b] for b in labels]\n",
    "        \n",
    "        racism = [i for i in range(len(labels)) if labels[i]==2]\n",
    "        sexism = [i for i in range(len(labels)) if labels[i]==1]\n",
    "        x_text = x_text + [x_text[x] for x in racism]*(oversampling_rate-1)+ [x_text[x] for x in sexism]*(oversampling_rate-1)\n",
    "        labels = labels + [2 for i in range(len(racism))]*(oversampling_rate-1) + [1 for i in range(len(sexism))]*(oversampling_rate-1)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        NUM_CLASSES = 2\n",
    "        bully = [i for i in range(len(labels)) if labels[i]==1]\n",
    "        x_text = x_text + [x_text[x] for x in bully]*(oversampling_rate-1)\n",
    "        labels = list(labels) + [1 for i in range(len(bully))]*(oversampling_rate-1)\n",
    "\n",
    "    print(\"Counter after oversampling\")\n",
    "    from collections import Counter\n",
    "    print(Counter(labels))\n",
    "    \n",
    "    filter_data = []\n",
    "    for text in x_text:\n",
    "        filter_data.append(\"\".join(l for l in text if l not in string.punctuation))\n",
    "        \n",
    "    return x_text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, x_text, labels, model_type, vector_type, embed_size, max_document_length=None):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split( x_text, labels, random_state=121, test_size=0.10)\n",
    "    \n",
    "    if(max_document_length==None):\n",
    "        post_length = np.array([len(x.split(\" \")) for x in x_text])\n",
    "        if(data != \"twitter\"):\n",
    "            max_document_length = int(np.percentile(post_length, 95))\n",
    "        else:\n",
    "            max_document_length = max(post_length)\n",
    "        print(\"Document length : \" + str(max_document_length))\n",
    "    \n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, MAX_FEATURES)\n",
    "    vocab_processor = vocab_processor.fit(x_text)\n",
    "\n",
    "    trainX = np.array(list(vocab_processor.transform(X_train)))\n",
    "    testX = np.array(list(vocab_processor.transform(X_test)))\n",
    "\n",
    "    vocab_size = len(vocab_processor.vocabulary_)\n",
    "    print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "\n",
    "    vocab = vocab_processor.vocabulary_._mapping\n",
    "    \n",
    "    trainY = np.asarray(Y_train)\n",
    "    testY = np.asarray(Y_test)\n",
    "        \n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "\n",
    "    trainY = to_categorical(trainY, nb_classes=NUM_CLASSES)\n",
    "    testY = to_categorical(testY, nb_classes=NUM_CLASSES)\n",
    "\n",
    "    print(\"Running Model: \" + model_type + \" with word vector initiliazed with \" + vector_type + \" word vectors.\")\n",
    "    model = get_model(model_type, trainX.shape[1], vocab_size, embed_size, NUM_CLASSES, LEARN_RATE)\n",
    "\n",
    "    if(model_type == 'cnn'):\n",
    "        if(vector_type!=\"random\"):\n",
    "            print(\"Word vectors used: \" + vector_type)\n",
    "            embeddingWeights = tflearn.get_layer_variables_by_name('EmbeddingLayer')[0]\n",
    "            model.set_weights(embeddingWeights, map_embedding_weights(get_embeddings_dict(vector_type, embed_size, data), vocab, embed_size))\n",
    "            model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
    "        else:\n",
    "            model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        if(vector_type!=\"random\"):\n",
    "            print(\"Word vectors used: \" + vector_type)\n",
    "            model.layers[0].set_weights([map_embedding_weights(get_embeddings_dict(vector_type, embed_size, data), vocab, embed_size)])\n",
    "            model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
    "                  verbose=1)\n",
    "        else:\n",
    "            model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
    "                  verbose=1)\n",
    "            \n",
    "    precision, recall, f1_score = evaluate_model(model, testX, testY)\n",
    "    \n",
    "    model_dict = {\n",
    "        \"model\": model,\n",
    "        \"testX\": testX,\n",
    "        \"testY\": testY,\n",
    "        \"trainX\" : trainX,\n",
    "        \"trainY\" : trainY,\n",
    "        \"vocab\": vocab_processor,\n",
    "        \"length\": max_document_length,\n",
    "        \"data\": data\n",
    "    }\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning_1(dict_1, dict_2, model_type=None, vector_type=None, embed_size=None):\n",
    "    \n",
    "    model = dict_1[\"model\"]\n",
    "    length = dict_1[\"length\"]\n",
    "    vocab1 = dict_1[\"vocab\"]\n",
    "    vocab2 = dict_2[\"vocab\"]\n",
    "    testX = dict_2[\"testX\"]\n",
    "    testY = dict_2[\"testY\"]\n",
    "    \n",
    "    temp = list(vocab2.reverse(testX))\n",
    "    testX  = np.array(list(vocab1.transform(temp)))\n",
    "    testX = pad_sequences(testX, maxlen=length, value=0.)\n",
    "    \n",
    "    evaluate_model(model, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_embedding_trained_weights(embed, vocab_1, vocab_2):\n",
    "    vocab_size = len(vocab_2)\n",
    "    embeddingWeights = np.zeros((vocab_size , embed.shape[1]))\n",
    "    n = 0\n",
    "    words_missed = []\n",
    "    for k, v in vocab_2.iteritems():\n",
    "        try:\n",
    "            embeddingWeights[v] = embed[vocab_1[k]]\n",
    "        except:\n",
    "            n += 1\n",
    "            words_missed.append(k)\n",
    "            pass\n",
    "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
    "    return embeddingWeights\n",
    "\n",
    "def transfer_learning_2(dict_1, dict_2, model_type, vector_type, embed_size):\n",
    "    trainX = dict_2[\"trainX\"]\n",
    "    trainY = dict_2[\"trainY\"]\n",
    "    testX = dict_2[\"testX\"]\n",
    "    testY = dict_2[\"testY\"]\n",
    "\n",
    "    EPOCHS = 5\n",
    "    BATCH_SIZE = 128\n",
    "    vocab_processor = dict_2[\"vocab\"]\n",
    "    vocab_size = len(vocab_processor.vocabulary_)\n",
    "    print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "\n",
    "    vocab = vocab_processor.vocabulary_._mapping\n",
    "\n",
    "    print(\"Running Model: \" + model_type + \" with word vector initiliazed with word vectors trained on dataset 1.\")\n",
    "    model = get_model(model_type, trainX.shape[1], vocab_size, embed_size, NUM_CLASSES, LEARN_RATE)\n",
    "\n",
    "    if(model_type == 'cnn'):\n",
    "        embeddingWeights = tflearn.get_layer_variables_by_name('EmbeddingLayer')[0]\n",
    "        embed = dict_1[\"model\"].get_weights(embeddingWeights)\n",
    "        model.set_weights(embeddingWeights, map_embedding_trained_weights(embed, dict_1[\"vocab\"].vocabulary_._mapping, vocab))\n",
    "        model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        embed = dict_1[\"model\"].layers[0].get_weights()[0]\n",
    "        model.layers[0].set_weights([map_embedding_trained_weights(embed, dict_1[\"vocab\"].vocabulary_._mapping, vocab)])\n",
    "        model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
    "              verbose=1)\n",
    "\n",
    "    evaluate_model(model, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning_3(dict_1, dict_2, model_type, vector_type, embed_size):\n",
    " \n",
    "    trainX = dict_2[\"trainX\"]\n",
    "    trainY = dict_2[\"trainY\"]\n",
    "    testX = dict_2[\"testX\"]\n",
    "    testY = dict_2[\"testY\"]\n",
    "\n",
    "    vocab_processor = dict_2[\"vocab\"]\n",
    "    vocab_size = len(vocab_processor.vocabulary_)\n",
    "    print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "\n",
    "    vocab = vocab_processor.vocabulary_._mapping\n",
    "\n",
    "    print(\"Running Model: \" + model_type + \" with word vector initiliazed with word vectors trainned on dataset 1.\")\n",
    "    model = get_model(model_type, trainX.shape[1], vocab_size, embed_size, NUM_CLASSES, LEARN_RATE)\n",
    "\n",
    "    EPOCHS = 5\n",
    "    BATCH_SIZE = 128\n",
    "    \n",
    "    if(model_type == 'cnn'):\n",
    "\n",
    "        embeddingWeights = tflearn.get_layer_variables_by_name('EmbeddingLayer')[0]\n",
    "        embed = dict_1[\"model\"].get_weights(embeddingWeights)\n",
    "        model.set_weights(embeddingWeights, map_embedding_trained_weights(embed, dict_1[\"vocab\"].vocabulary_._mapping, vocab))\n",
    "\n",
    "        layer1Weights = tflearn.get_layer_variables_by_name('layer_1')[0]\n",
    "        model.set_weights(layer1Weights, dict_1[\"model\"].get_weights(layer1Weights))\n",
    "\n",
    "        layer2Weights = tflearn.get_layer_variables_by_name('layer_2')[0]\n",
    "        model.set_weights(layer2Weights, dict_1[\"model\"].get_weights(layer2Weights))\n",
    "\n",
    "        layer3Weights = tflearn.get_layer_variables_by_name('layer_1')[0]\n",
    "        model.set_weights(layer3Weights, dict_1[\"model\"].get_weights(layer3Weights))\n",
    "\n",
    "        fcWeights = tflearn.get_layer_variables_by_name('layer_1')[0]\n",
    "        model.set_weights(fcWeights, dict_1[\"model\"].get_weights(fcWeights))\n",
    "\n",
    "        model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "    elif(model_type == 'blstm_attention'):\n",
    "        embed = dict_1[\"model\"].layers[0].get_weights()[0]\n",
    "        model.layers[0].set_weights([map_embedding_trained_weights(embed, dict_1[\"vocab\"].vocabulary_._mapping, vocab)])\n",
    "        model.layers[2].set_weights(dict_1[\"model\"].layers[2].get_weights())\n",
    "        model.layers[3].set_weights(dict_1[\"model\"].layers[3].get_weights())\n",
    "        model.layers[5].set_weights(dict_1[\"model\"].layers[5].get_weights())\n",
    "        model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
    "              verbose=1)\n",
    "    else:\n",
    "        embed = dict_1[\"model\"].layers[0].get_weights()[0]\n",
    "        model.layers[0].set_weights([map_embedding_trained_weights(embed, dict_1[\"vocab\"].vocabulary_._mapping, vocab)])\n",
    "        model.layers[2].set_weights(dict_1[\"model\"].layers[2].get_weights())\n",
    "        model.layers[4].set_weights(dict_1[\"model\"].layers[4].get_weights())\n",
    "        model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
    "              verbose=1)\n",
    "\n",
    "    evaluate_model(model, testX, testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = \"formspring\"\n",
    "data_2 = \"twitter\"\n",
    "data_3 = \"wiki\"\n",
    "model_type =\"blstm\"\n",
    "vector_type = \"glove\"\n",
    "embed_size = 50\n",
    "oversampling_rate = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: data/formspring_data.pkl\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d17c74fc7352>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moversampling_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdict_1\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moversampling_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdict_2\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-60bf763bc61b>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(data, oversampling_rate)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moversampling_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mx_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"twitter\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-bc4f2fcb0873>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading data from file: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mx_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "x_text, labels = get_data(data_1, oversampling_rate)\n",
    "dict_1 =  train(data_1, x_text, labels, model_type, vector_type, embed_size)\n",
    "\n",
    "x_text, labels = get_data(data_2, oversampling_rate)\n",
    "dict_2 =  train(data_2, x_text, labels, model_type, vector_type, embed_size)\n",
    "\n",
    "x_text, labels = get_data(data_3, oversampling_rate)\n",
    "dict_3 =  train(data_3, x_text, labels, model_type, vector_type, embed_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ba88a22407ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m }\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdata_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdict_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dict_1' is not defined"
     ]
    }
   ],
   "source": [
    "transfer_learning = {\n",
    "    1: transfer_learning_1,\n",
    "    2: transfer_learning_2,\n",
    "    3: transfer_learning_3\n",
    "}\n",
    "\n",
    "data_dict = [dict_1, dict_2, dict_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(data_dict, model_type, vector_type, embed_size, ind):\n",
    "    for i in range(len(data_dict)):\n",
    "        for j in range(i+1, len(data_dict)):\n",
    "            transfer_learning[ind](data_dict[i],data_dict[j], model_type, vector_type, embed_size) \n",
    "            transfer_learning[ind](data_dict[j],data_dict[i], model_type, vector_type, embed_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-3e321557ad9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data_dict' is not defined"
     ]
    }
   ],
   "source": [
    "get_results(data_dict, model_type, vector_type, embed_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-744ccbab92f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data_dict' is not defined"
     ]
    }
   ],
   "source": [
    "get_results(data_dict, model_type, vector_type, embed_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-54c46f428104>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data_dict' is not defined"
     ]
    }
   ],
   "source": [
    "get_results(data_dict, model_type, vector_type, embed_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
