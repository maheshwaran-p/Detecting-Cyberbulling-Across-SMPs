{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"ERROR: Please specify a correst model\")? (<ipython-input-2-8c30cad22229>, line 123)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-8c30cad22229>\"\u001b[1;36m, line \u001b[1;32m123\u001b[0m\n\u001b[1;33m    print \"ERROR: Please specify a correst model\"\u001b[0m\n\u001b[1;37m                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"ERROR: Please specify a correst model\")?\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Merge, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model,Sequential\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "\n",
    "\n",
    "def lstm_keras(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
    "#     K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim, trainable=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(LSTM(embed_size))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    print (model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def cnn(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
    "    tf.reset_default_graph()\n",
    "    network = input_data(shape=[None, inp_dim], name='input')\n",
    "    network = tflearn.embedding(network, input_dim=vocab_size, output_dim=embed_size, name=\"EmbeddingLayer\")\n",
    "    network = dropout(network, 0.25)\n",
    "    branch1 = conv_1d(network, embed_size, 3, padding='valid', activation='relu', regularizer=\"L2\", name=\"layer_1\")\n",
    "    branch2 = conv_1d(network, embed_size, 4, padding='valid', activation='relu', regularizer=\"L2\", name=\"layer_2\")\n",
    "    branch3 = conv_1d(network, embed_size, 5, padding='valid', activation='relu', regularizer=\"L2\", name=\"layer_3\")\n",
    "    network = merge([branch1, branch2, branch3], mode='concat', axis=1)\n",
    "    network = tf.expand_dims(network, 2)\n",
    "    network = global_max_pool(network)\n",
    "    network = dropout(network, 0.50)\n",
    "    network = fully_connected(network, num_classes, activation='softmax', name=\"fc\")\n",
    "    network = regression(network, optimizer='adam', learning_rate=learn_rate,\n",
    "                         loss='categorical_crossentropy', name='target')\n",
    "    \n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0)\n",
    "    return model\n",
    "\n",
    "\n",
    "def blstm(inp_dim,vocab_size, embed_size, num_classes, learn_rate):   \n",
    "#     K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim, trainable=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Bidirectional(LSTM(embed_size)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class AttLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1],),\n",
    "                                      initializer='random_normal',\n",
    "                                      trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def blstm_atten(inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
    "#     K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embed_size, input_length=inp_dim))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Bidirectional(LSTM(embed_size, return_sequences=True)))\n",
    "    model.add(AttLayer())\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    adam = optimizers.Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "   \n",
    "    return model\n",
    "\n",
    "def get_model(m_type,inp_dim, vocab_size, embed_size, num_classes, learn_rate):\n",
    "    if m_type == 'cnn':\n",
    "        model = cnn(inp_dim, vocab_size, embed_size, num_classes, learn_rate)\n",
    "    elif m_type == 'lstm':\n",
    "        model = lstm_keras(inp_dim, vocab_size, embed_size, num_classes, learn_rate)\n",
    "    elif m_type == \"blstm\":\n",
    "        model = blstm(inp_dim)\n",
    "    elif m_type == \"blstm_attention\":\n",
    "        model = blstm_atten(inp_dim, vocab_size, embed_size, num_classes, learn_rate)\n",
    "    else:\n",
    "        print \"ERROR: Please specify a correst model\"\n",
    "        return None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
